{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Planteamiento del Proyecto\n",
    "\n",
    "# Representación de Datos\n",
    "\n",
    "# Excepción personalizada\n",
    "class ErrorFormatoDatosInvalido(Exception):\n",
    "    \"\"\"Excepción para errores en el formato de los datos de entrada.\"\"\"\n",
    "    def __init__(self, mensaje=\"Formato de datos inválido. Se esperan arrays de NumPy.\"):\n",
    "        self.mensaje = mensaje\n",
    "        super().__init__(self.mensaje)\n",
    "\n",
    "def generar_datos_sinteticos(num_muestras=100, rango_x=(0, 10), std_ruido=1.0):\n",
    "    \"\"\"\n",
    "    Genera datos sintéticos para regresión lineal simple: y = 4 + 3x + ε.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(num_muestras, int) or num_muestras <= 0:\n",
    "            raise ValueError(\"num_muestras debe ser un entero positivo.\")\n",
    "        if not isinstance(rango_x, tuple) or len(rango_x) != 2 or rango_x[0] >= rango_x[1]:\n",
    "            raise ValueError(\"rango_x debe ser una tupla (min, max) con min < max.\")\n",
    "        if not isinstance(std_ruido, (int, float)) or std_ruido < 0:\n",
    "            raise ValueError(\"std_ruido debe ser un número no negativo.\")\n",
    "\n",
    "        np.random.seed(42) # Para reproducibilidad\n",
    "        x = np.random.uniform(rango_x[0], rango_x[1], num_muestras).reshape(-1, 1)\n",
    "        ruido = np.random.normal(0, std_ruido, num_muestras).reshape(-1, 1)\n",
    "        y = 4 + 3 * x + ruido\n",
    "        return x, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar datos sintéticos: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generar datos y representarlos con NumPy\n",
    "try:\n",
    "    X_datos_original, y_datos = generar_datos_sinteticos(num_muestras=100)\n",
    "    # Añadir columna de unos a X para el término de sesgo (intercepto)\n",
    "    X_con_sesgo = np.c_[np.ones((X_datos_original.shape[0], 1)), X_datos_original]\n",
    "    print(\"Datos Generados\")\n",
    "    print(\"X (primeros 5): \", np.round(X_datos_original[:5].flatten(), 2))\n",
    "    print(\"y (primeros 5): \", np.round(y_datos[:5].flatten(), 2))\n",
    "except ErrorFormatoDatosInvalido as e:\n",
    "    print(f\"Error de formato de datos: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado durante la generación de datos: {e}\")\n",
    "finally:\n",
    "    print(\"Finalizada la sección de generación de datos.\")\n",
    "\n",
    "\n",
    "# Resolución del Modelo (Cálculo Cerrado - Ecuación Normal)\n",
    "\n",
    "def resolver_ecuacion_normal(X, y):\n",
    "    \"\"\"\n",
    "    Resuelve la regresión lineal utilizando la ecuación normal (β = (X^T X)^-1 X^T y).\n",
    "    Maneja excepciones de matriz singular.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validar formato de datos de entrada\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n",
    "            raise ErrorFormatoDatosInvalido(\"Los datos de entrada para la ecuación normal deben ser arrays de NumPy.\")\n",
    "        if X.ndim != 2 or y.ndim != 2 or X.shape[0] != y.shape[0]:\n",
    "            raise ErrorFormatoDatosInvalido(\"X debe ser un array 2D y 'y' un array 2D con el mismo número de filas.\")\n",
    "\n",
    "        X_transpuesta = X.T\n",
    "        try:\n",
    "            # Calcular (X^T X)^-1\n",
    "            XtX_inversa = np.linalg.inv(X_transpuesta @ X)\n",
    "        except np.linalg.LinAlgError:\n",
    "            raise ValueError(\"La matriz (X^T X) es singular, no se puede invertir. Pruebe con más datos o elimine características linealmente dependientes.\")\n",
    "\n",
    "        # Calcular beta (parámetros)\n",
    "        beta = XtX_inversa @ X_transpuesta @ y\n",
    "        return beta\n",
    "    except ErrorFormatoDatosInvalido as e:\n",
    "        print(f\"Error de formato de datos en la función 'resolver_ecuacion_normal': {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        print(f\"Error en la ecuación normal: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado durante la resolución con la ecuación normal: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    beta_ecuacion_normal = resolver_ecuacion_normal(X_con_sesgo, y_datos)\n",
    "    print(\"\\nParámetros del Modelo (Ecuación Normal)\")\n",
    "    print(f\"Sesgo (theta_0): {beta_ecuacion_normal[0, 0]:.4f}\")\n",
    "    print(f\"Pendiente (theta_1): {beta_ecuacion_normal[1, 0]:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo calcular con la ecuación normal debido a: {e}\")\n",
    "    beta_ecuacion_normal = None # Asegurar que la variable esté definida para evitar errores posteriores\n",
    "finally:\n",
    "    print(\"Finalizada la sección de ecuación normal.\")\n",
    "\n",
    "\n",
    "# Optimización Iterativa (Método de Descenso de Gradiente)\n",
    "\n",
    "def calcular_costo(X, y, theta):\n",
    "    \"\"\"\n",
    "    Calcula la función de costo (Error Cuadrático Medio - MSE).\n",
    "    J(θ) = 1/(2m) * Σ(h_θ(x^(i)) - y^(i))^2\n",
    "    \"\"\"\n",
    "    try:\n",
    "        m = len(y)\n",
    "        predicciones = X @ theta\n",
    "        costo = (1/(2*m)) * np.sum(np.square(predicciones - y))\n",
    "        return costo\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular el costo: {e}\")\n",
    "        raise\n",
    "\n",
    "def descenso_gradiente(X, y, tasa_aprendizaje=0.01, num_iteraciones=1500):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo de Descenso de Gradiente para regresión lineal.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validar formato de datos de entrada\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n",
    "            raise ErrorFormatoDatosInvalido(\"Los datos de entrada para descenso de gradiente deben ser arrays de NumPy.\")\n",
    "        if X.ndim != 2 or y.ndim != 2 or X.shape[0] != y.shape[0]:\n",
    "            raise ErrorFormatoDatosInvalido(\"X debe ser un array 2D y 'y' un array 2D con el mismo número de filas.\")\n",
    "        if not isinstance(tasa_aprendizaje, (int, float)) or tasa_aprendizaje <= 0:\n",
    "            raise ValueError(\"tasa_aprendizaje debe ser un número positivo.\")\n",
    "        if not isinstance(num_iteraciones, int) or num_iteraciones <= 0:\n",
    "            raise ValueError(\"num_iteraciones debe ser un entero positivo.\")\n",
    "\n",
    "        m = len(y)\n",
    "        theta = np.zeros((X.shape[1], 1)) # Inicializar parámetros (sesgo y pendiente) con ceros\n",
    "        historial_costo = []\n",
    "\n",
    "        for i in range(num_iteraciones):\n",
    "            predicciones = X @ theta\n",
    "            errores = predicciones - y\n",
    "            # Gradiente: (1/m) * X^T * errores\n",
    "            gradiente = (1/m) * X.T @ errores\n",
    "            theta = theta - tasa_aprendizaje * gradiente\n",
    "            historial_costo.append(calcular_costo(X, y, theta))\n",
    "        return theta, historial_costo\n",
    "    except ErrorFormatoDatosInvalido as e:\n",
    "        print(f\"Error de formato de datos en la función 'descenso_gradiente': {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        print(f\"Error en descenso de gradiente: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado durante la ejecución de descenso de gradiente: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    theta_descenso_gradiente, historial_costo_descenso_gradiente = descenso_gradiente(X_con_sesgo, y_datos, tasa_aprendizaje=0.01, num_iteraciones=1500)\n",
    "    print(\"\\nParámetros del Modelo (Descenso de Gradiente)\")\n",
    "    print(f\"Sesgo (theta_0): {theta_descenso_gradiente[0, 0]:.4f}\")\n",
    "    print(f\"Pendiente (theta_1): {theta_descenso_gradiente[1, 0]:.4f}\")\n",
    "\n",
    "    if beta_ecuacion_normal is not None:\n",
    "        costo_final_ecuacion_normal = calcular_costo(X_con_sesgo, y_datos, beta_ecuacion_normal)\n",
    "        print(f\"Costo final (Ecuación Normal): {costo_final_ecuacion_normal:.4f}\")\n",
    "    else:\n",
    "        costo_final_ecuacion_normal = \"No calculado\"\n",
    "        print(\"Costo final (Ecuación Normal): No disponible debido a un error previo.\")\n",
    "\n",
    "    costo_final_descenso_gradiente = historial_costo_descenso_gradiente[-1]\n",
    "    print(f\"Costo final (Descenso de Gradiente): {costo_final_descenso_gradiente:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo calcular con descenso de gradiente debido a: {e}\")\n",
    "    theta_descenso_gradiente = None # Asegurar que la variable esté definida\n",
    "    historial_costo_descenso_gradiente = []\n",
    "finally:\n",
    "    print(\"Finalizada la sección de descenso de gradiente.\")\n",
    "\n",
    "# 2. Visualización y Análisis\n",
    "\n",
    "# Graficar el conjunto de datos junto con la recta de regresión\n",
    "if theta_descenso_gradiente is not None and beta_ecuacion_normal is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_datos_original, y_datos, label='Datos Sintéticos', alpha=0.6)\n",
    "\n",
    "    # Línea de regresión de Ecuación Normal\n",
    "    x_linea = np.array([X_datos_original.min(), X_datos_original.max()]).reshape(-1, 1)\n",
    "    X_linea_con_sesgo = np.c_[np.ones((x_linea.shape[0], 1)), x_linea]\n",
    "    y_pred_ecuacion_normal = X_linea_con_sesgo @ beta_ecuacion_normal\n",
    "    plt.plot(x_linea, y_pred_ecuacion_normal, color='red', label=f'Regresión (Ecuación Normal): y = {beta_ecuacion_normal[0,0]:.2f} + {beta_ecuacion_normal[1,0]:.2f}x')\n",
    "\n",
    "    # Línea de regresión de Descenso de Gradiente\n",
    "    y_pred_descenso_gradiente = X_linea_con_sesgo @ theta_descenso_gradiente\n",
    "    plt.plot(x_linea, y_pred_descenso_gradiente, color='green', linestyle='--', label=f'Regresión (Descenso de Gradiente): y = {theta_descenso_gradiente[0,0]:.2f} + {theta_descenso_gradiente[1,0]:.2f}x')\n",
    "\n",
    "    plt.xlabel('Variable Independiente (x)')\n",
    "    plt.ylabel('Variable Dependiente (y)')\n",
    "    plt.title('Regresión Lineal Simple: Comparación de Métodos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo se pueden generar gráficos de regresión porque uno o ambos métodos fallaron.\")\n",
    "\n",
    "# Generar un gráfico que muestre la evolución de la función de costo\n",
    "if historial_costo_descenso_gradiente:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(historial_costo_descenso_gradiente)), historial_costo_descenso_gradiente, color='blue')\n",
    "    plt.xlabel('Número de Iteraciones')\n",
    "    plt.ylabel('Función de Costo (MSE)')\n",
    "    plt.title('Evolución de la Función de Costo en Descenso de Gradiente (GD)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo se puede generar el gráfico de evolución del costo porque el descenso de gradiente falló o no se ejecutó.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
